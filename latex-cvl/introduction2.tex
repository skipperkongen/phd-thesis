% !TEX root = ./ICDE14_conf_full_296.tex
\section{Introduction}
A problem in data management is generalizing spatial datasets for visualization on zoomable maps. A good map should satisfy two main requirements: it should be visually legible and should be a good representation of the dataset that is visualized. These two conflicting goals should be balanced with respect to the scale and purpose of a given map. In a zoomable map, at dataset is visualized at many different scales, implying that the dataset must be generalized for each of these scales. 

Generalization is inherently a process of reduction. The goal is to reduce visual ``clutter'' which is usually achieved by some kind of reduction of the data. When datasets contain millions or even billions of records, and there is no obvious way to aggregate them~\cite{haunert2006landcover}, the best option seems to be to filter out a subset of the records~\cite{sarma2012fusiontables}. This should ideally be combined with some means to prioritize which records will remain. Generalization by omitting records is known in the generalization literature as \emph{selection}~\cite{weibel1999generalising}.
%Other options include \emph{displacement} and \emph{aggregation} of data, but these techniques are not always applicable or even meaningful. For instance it is not immediately obvious how to aggregate geocoded images or messages from social media.

Fully automatic selection of spatial records is increasingly relevant due to the constant rise of geocoded data on the web, such as images and messages from social media. This trend is coupled with a rising demand to understand and visualize this data. Exciting new areas driving the demand include social media, factivism and data journalism~\cite{cohen2011journalism}. 
%In these areas there is a constant need for visualizing new and often massive geospatial datasets.

To meet the demand, a selection method should be able to handle big spatial datasets and be usable by people with little or no programming experience. This implies the need to express selection in terms understood by novice users, and in a concise way. It also implies the need for an implementation that, if need be, can be scaled effortlessly.

Unfortunately, current approaches to selection fall short in one or both of these respects. Systems have been developed which apparently work fully automatically, but typically these are confined to the memory of a single machine (are not scalable) or the selection process is best very time consuming to control for users (are not programmable)~\cite{sarma2012fusiontables}. We address these shortcomings by several innovations, which are combined in our solution.

To address the programmability issue, we have designed a declarative language, CVL, which is arguably simple enough to be used by novice programmers. The core idea of this language is that users should express selection in terms that they understand, such as specifying constraints and indicating the importance of records. Users should not express selection in terms that are hard to grasp, such as the algorithms used to compute the results. As a result of these design choices, programs written in CVL are concise and fully declarative.

To address the scalability issue, we make use of existing algorithms and existing database technology. On the algorithmic side, the program is mapped to a well-known optimization problem, for which good algorithms exist~\cite{rajagopalan1998primal}. Because spatial data is often stored in a database with powerful spatial extensions installed, it is a natural idea to use the database as a runtime for executing the selection process. On the evaluation side, we compile programs written in CVL into database programs expressed largely in SQL. This allows CVL programs to be executed on highly scalable databases, which could theoretically include parallel databases. 

The benefits of moving \emph{code-to-data} instead of vice-versa include better utilization of network bandwidth and making use of existing indexing and partitioning of the spatial data~\cite{Guttman1984:RTree,Hellerstein1995:GiST}. We call our approach \emph{Declarative Cartography}.

\vspace{5em}

We make the following four contributions in this paper:
\begin{enumerate}
\item We present a declarative language, Cartographic Visualization Language (CVL, pronounced ``civil''), for generalizing spatial datasets. CVL is designed to be simple and concise to use for novice programmers. The CVL language was designed in collaboration with the Danish Geodata Agency~\footnote{\texttt{http://www.gst.dk/English/}} and employees at the engineering company Grontmij~\footnote{\texttt{http://grontmij.dk/}} in Denmark.

\item We map the selection problem in map generalization to the well-known \emph{set multicover problem}~\cite{rajagopalan1998primal}, which makes constraints fully pluggable and allows reuse of well-known approximation algorithms~\cite{rajagopalan1998primal,vazirani2001approximation}.

\item We show how to fully evaluate CVL inside the database; this enables us to reuse basic database technology for data management and scalability. While CVL is designed to compile to a variety of engines~\cite{Stonebraker:2010:PDBMSvsMapReduce}, we present here an implementation using a relational database engine with spatial extensions.

\item We present experimental results for a variety of real datasets. The results show that the proposed approach has good performance and is able to produce useful map generalizations.
\end{enumerate}

In Section~\ref{sec:background}, we define the selection problem in map generalization. In Section~\ref{sec:cvl:language}, we introduce the CVL language. In Section~\ref{sec:optimizationmodel}, we formalize the multi-scale filtering problem which is based on a mapping to the set multicover problem, and we revisit algorithms for this problem in Section~\ref{sec:algorithms}. In Section~\ref{sec:implementation}, we discuss the compilation procedure that enables us to run CVL on a relational database backend. Experimental results are presented in Section~\ref{sec:experimental}, and finally related work is summarized in Section~\ref{sec:related}.
