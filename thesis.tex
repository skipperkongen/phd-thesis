\documentclass[11pt, oneside]{report}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}

% Copied from Marcos Style:
\setlength{\parindent}{0px}
\setlength{\parskip}{1em}

\title{Declarative Multi-Scale Map Visualization and Efficient Serving of Zoomable Geographical Maps}
\author{Pimin Konstantin Kefaloukos}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

\tableofcontents

\chapter{Introduction}

Case Study, P1 enables P2, Mention where related work is (which parts/chapters)


\section{Motivation}
% Why are they important? Use cases.
Zoomable geographical maps are important to people because they facilitate spatial insights and support spatial decision making. For example, journalists can use maps to deliver spatial insights to their readers when reporting about a war, where the location of troops, resources and refugees is important. Tourist exploring an unfamiliar city can use location based services (LBS) running on their mobile devices to locate a near-by restaurants and shops when they suddenly feel hungry or in the mood for shopping. Importantly, maps should be made \emph{zoomable} whenever data can not be legibly visualized within a single frame, because otherwise useful information might be rendered useless. Zoomable maps allows users to explore vast amounts of spatial information at different focal points and levels of abstraction. For example, points can be selected and aggregated at low scales, and fully shown only at high scales, which enables switching between overview mode and detailed exploration mode.

% What are maps and how are they used?
Web-based multiscale maps have recently attracted millions of users, driven by an explosive growth in the availability of useful spatial data (e.g. commercial, social, environmental, scientific and political data) and by near-ubiquitous web access in many parts of the world. The opportunity to access maps over the web is bolstered by soaring world-wide sales of mobile devices (e.g. smart phones, tablets and laptops). In combination, millions of users now have the ability and motivation to spontaneously engage maps -- anywhere and anytime.

% Views and assumptions
We need some simplifying \emph{views} and \emph{assumptions} in order to scope our analysis of the complex pipeline needed for end-to-end map delivery. First, we adopt the view that the mapping pipeline consists of two high-level tasks: \emph{map production} and \emph{map consumption}. Second, we assume that map production in comprised of the dual tasks of \emph{data abstraction} (e.g. selection and aggregation) and \emph{visual abstraction} (e.g. styling)~\cite{stolte2003multiscale}. Third, we assume that map consumption occurs in the \emph{client-server model}.

% What are the challenges: caused by high traffic and constant surfacing of new and often big datasets.
XXX challenges of map production are (x), (xx), (xxx) saving manual time, saving computation time, powerful expressibility. YYY challenges of map consumption are (y), (yy), (yyy) high availability and high performance.

% Variables
What are the variables? Relevant metrics should be quantified in service level agreements (SLA) and monitored continuously. For a given fixed SLA, we can mimimize the monetary cost (e.g. salaries, hardware and license expenses, energy bills) of a fixed SLA generally increases with the number of \emph{concurrent users} and data \emph{volume}, \emph{variety}, and \emph{velocity} (``the Three Vs of Big Data'').


% Map production is synonymous with creating good maps, which does not imply consumption of maps. Map consumption is synonymous with users browsing maps on their devices, which does not imply good maps. 
% , with map services running on servers and map clients running on user devices (e.g. mobile devices)

, i.e. , three key challenges face map designers and system engineers: Map designers must (a) address the problem of \emph{data abstraction} (e.g. selection and aggregation) and (b) address the problem of  \emph{visual abstraction} (e.g. choice of graphical styles)~\cite{stolte2003multiscale}. This must be done at the rapid pace that new zoomable maps are needed~\cite{lomet2012warstories}. Third, system engineers must (c) manage \emph{brutal query workloads} (e.g. millions of concurrent users).

\emph{Idea}: motivate preprocessing and vector tiles with data from monitoring on servage that shows that ``dumb'' files have better performance and availability.

\section{Architectures for Spatial Information Pipelines}

\subsection{Types of Geographical Maps}

\subsection{Types of Pipelines for Maps}
% review various pipelines for maps. This motivates 

\section{Design Objectives}
In a nutshell, maps must communicate important spatial information in a useful way to their intended audience~\cite{robinson1982early}. While this axiom is certainly true of all maps, it is not concrete enough to effectively guide the map design process. Ideally, it needs to be broken down into a list of requirements that can be checked using quantitative measures. In the following sections, we will review requirements for maps that have been stated.

\subsection{iOS6 Apple Maps Requirements}
Six design objectives have been used to evaluate the iOS6 Apple Maps API~\cite{nutanong2012multiresolution}:
\begin{enumerate}
\item \emph{Minimize overlap}. In later sections show how this requirement is implemented as proximity constraint
\item \emph{Respect relative importance of entries}. Implemented by weighting.
\item \emph{Maximize spatial fullness}. Implemented by SMCP model.
\item \emph{Provide panning/zooming consistency}. Implemented by recursive SQL, ladder approach.
\item \emph{Enable efficient sampling}. Implemented by preprocessing approach.
\item \emph{Support filtering conditions}. Problem here, also exhibited by fusiontables.
\end{enumerate}

\subsection{OpenScienceMaps Requirements}
Check requirementes from OpenScienceMap~\cite{schmid2013opensciencemap}. My notes: Good idea to let rank have impact on proximity constraint: Lower ranked objects require more separation, which implies that conflicts are directed. Question: can the SELECT-PLACES algorithm be implemented in terms of my fundamental constraints and objectives? If not, make sure I can. The places selected by CVL are "too" globally optimal. The important criteria is including locally relevant places, not having a globally optimal result. A five star restaurant should never loose to a four star restaurant!

\subsection{GST Requirements}
Place name implies that we must support star approach as well as ladder approach (se email med Flemming). These will motivate CVL 2.


\subsection{Constant Information Density}
In 1961, the cartographer Friedrich T\"{o}pfer proposed the \emph{Principle of Selection}, which states that the number of objects per display unit should be constant -- regardless of scale~\cite{topfer1966principles}. The soundness of the principle has been verified empirically, e.g. by counting the number of settlements shown per unit display area on different hand-crafted maps of Scotland~\cite{topfer1966principles}. 

In 1998, Woodruff et al. revived the principle for multi-scale database visualizations as the \emph{Principle of Constant Information Density}~\cite{woodruff1998constant}. While these principles differ in name, the core meaning is the same, namely that information density should remain constant over change in scale. A small nuance is in the density metrics that are used, i.e. how information density is quantified.

It is not difficult to understand why the Principle of Constant Information Density must be true. Computer screens and map charts have a fixed area on which to rendering information objects. There is a physical bound on how much information can be represented per unit display area (e.g. a few thousand pixels per $cm^2$ on a computer screen). Also, the human eye cannot see things that are too small or distinguish things that are too close together.

\subsubsection{Metrics}

\subsection{Requirements for iOS 6 Maps}
Apple iPhone 6 maps requirements listed in SELECT-DISTINCT paper, and in duking it out paper. Constant Information density




\section{Zoomable Geographical Maps on the Web}
\subsection{Definitions}
\subsection{Key Challenges}

\section{Overview of State-of-the-Art in Geographical Maps on the Web}
\subsection{Vector Tiles and Client-side Rendering}


\section{Research Gaps}
\subsection{Declarative Design}
\subsection{Workload Prediction}

\section{Contributions of the Dissertation}

\subsection{CVL1}
Brief summary and reference to Section.
\subsection{CVL2}
Brief summary and reference to Section.
\subsection{TileHeat}
Brief summary and reference to Section.


\part{Automatic Multiscale Data Abstraction}

\chapter{Survey of Automatic Multiscale Data Abstraction}

\section{Online processing}

\section{Preprocessing}

\chapter{CVL1}
Closing the gap, part a

\chapter{CVL2} 
Closing the gap, part b

\part{Serving Web Maps}

\chapter{Survey of Map Serving}

\section{Getting to Keyed Data for Maps}

\section{Caching}

\section{Prediction}

\section{Data Partitioning}

\section{Replication}

\section{Consistency}

\section{Key-value Stores}

\section{Classic Web Serving Infrastructures}

\chapter{Case Study: Danish Geodata Agency}

\section{Gap between the State of the Art, and the Agency}

\chapter{TileHeat}
TileHeat (closing the gap, step 1), deploying a caching infrastructure




\chapter{Thesis Topics}

\begin{description}
\item [System Types:] Databases (OLAP, OLTP), ETL, GIS, Data Processing (MapReduce), Caches, Web Servers
\item [Data Models:] Relational, Graph, XML Document, Flat records 
\item [Programming models:] Declarative, Imperiative, Object-Oriented, Functional (MapReduce)
\end{description}

\chapter{Overview of Geographical Maps}

These are the orthogonal dimensions of geographical map design and use.

% Two roles in maps
Two roles in geographical maps: map designer and map user

% Two types of data sources and deployments
Two types of data sources: stored (e.g. database, files) and streaming (e.g. sensors, dynamic databases). Both of these data source types can be local or remote.

% Two types of "liveliness"
Data can be static or dynamic

% Two extreme types of data authoring
Data can be authored automatically (e.g. captured by sensors) or manually (person digitizing a street).

% Two extreme types of map authoring
Maps can be automatically created using algorithms or maps can be manually designed.

% Two types of projections
There are two types of projections: physical (e.g. mountain map) and logical (e.g. subway map).

% Two types of data types
Two types of data types: raster and vector. Two subtypes of raster: orthophoto and grid data. Four subtypes of vector data: points, lines, polygons and polyhedra.

% Two types of end-maps (the stuff that map users look at)
Image-based maps and vector-based maps. Both types can be either single or multi-scale maps.

% Two types of map materializations
Maps can either be materialized off-line or online.

\chapter{Literature Surveys}

Maps There are various paths to create maps.
Follow a (1) design map; (2) create map ; (3) serve map approach, especially with regard to survey topics covered. Each technology will be ranked in these three categories using a set of metrics: (a) monetary cost; (b) scalability; (c) availability; (4) ease of use.


\section{MapReduce for Maps}
% In this section I'll survey how Map Reduce is used for spatial data.


% Typical problem solved by MapReduce
% - Read a lot of data
% - Map: extract something you care about from each record
% - Shuffle and Sort
% - Reduce: aggregate, summarize, filter, or transform
% - Write the results
% Outline stays the same, map and reduce change to fit the problem
% \cite{dean2009lessons}
MapReduce is a programming model and parallel data processing system implemented by Google and later described in a highly-cited white paper~\cite{dean2008mapreduce}. While MapReduce was originally designed by Google to run an its shared-nothing cluster of thousands of commodity servers, it is now deployed on both shared-memory multicores~\cite{ranger2007multicore} and graphics processors (GPUs)~\cite{he2008mapreducegpu} and used by some of the worlds largest organizations. The wide-spread adoption of the MapReduce programming model is largely due to the open source Apache Hadoop implementation~\cite{apachehadoop}, which together with the simplicity of the programming mode has made it a popular choice for scalable data processing. 

The backbone of a MapReduce program is made up of two functions, map and reduce:

\begin{tabular}{l l l}
$map$ & $(k1, v1)$ & $\rightarrow list(k2, v2)$\\ 
$reduce$ & (k2, list(v2)) & $\rightarrow list(v2)$ \\
\end{tabular}

As a natural consequence of its popularity, MapReduce has been used for spatial use cases, e.g. for spatial data analytics and spatial Extract-Transform-Load (ETL). For example, MapReduce can be used as an ETL tool to compute a set of geographical map tiles from a spatial dataset (points, lines, polygons) before loading the tiles into a scalable key-value store~\cite{dean2009lessons}. This follows a five-step work flow typical of MapReduce programs (we will assume $n$ mappers and $m$ reducers):

\begin{enumerate}
\item The set of spatial features is randomly partitioned $n$ ways and forwarded to the $n$ mappers

\item For each feature, the map function computes the set of intersected map tiles (rectangular coordinate region) and emits a list of corresponding $\langle TileID, Feature \rangle$ pairs

\item In a shuffle and sort step, the $\langle TileID, Feature \rangle$ pairs are rearranged into lists such that contain all $Feature$ values that were grouped with a given $TileID$ key, and these lists are distributed evenly among the $m$ reducers

\item Each reducer receives a set of $\langle TileID, List(Feature) \rangle$ pairs and aggregates each $Feature$ list into a finished map tile for $TileID$

\item The set of map tiles is stored on a high-performance key-value store, ready for client map requests. 
\end{enumerate}

\subsection{MapReduce Languages}

While developers can directly write basic MapReduce code to solve a large class of spatial analytics problems (CITE SPATIAL JOIN IN MAP REDUCE), languages have been designed that accommodate developers who prefer a different programming style -- including higher-level declarative~\cite{thusoo2009hive} and procedural~\cite{olston2008pig,eldawy2014pigeon} styles. Higher-level languages, like HiveQL and Pig, are translated into lower-level programs expressed in terms of the map and reduce functions. These languages are part of database systems that are built on top of MapReduce, such as HadoopDB~\cite{abouzeid2009hadoopdb}, HadoopGIS~\cite{aji2013hadoopgis}, Pig~\cite{eldawy2014pigeon, olston2008pig} and Hive~\cite{thusoo2009hive}.

\subsection{MapReduce for Materializing Maps}

A parallel RDBMS (e.g. an in-memory column-store) is generally the most efficient way to analyze large datasets~\cite{pavlo2009comparison}. For this reason several large organizations including Google are now switching back to an RDBMS approach for their analytics (away from MapReduce)~\cite{melnik2011dremel}. This switch will probably remain sensible from a pure performance point of view, in spite of recent research effort into automatically optimizing MapReduce ~\cite{jahani2011mapreduceoptimization,floratou2011columnmapreduce}. Instead of competing with a parallel RDBMS for the spot as analytics engine, MapReduce will most likely play a role together with a parallel RDMBS in an composite analytics architecture, i.e. as an \emph{extract-transform-load} (ETL) tool in front of a parallel RDBMS~\cite{stonebraker2010friendsorfoes}. The previously mentioned case of generating map tiles using MapReduce is an example of using MapReduce as an ETL tool in-front of a key-value store.


While MapReduce frameworks can be used to implement the guts of a database, an recent trend is to use MapReduce as an  Several large organizations including Google have taking this route lately, and in a sense 


In a spatial setting, geographical map tiles can be created using MapReduce by following a five-step work flow that is typical of such MapReduce programs. 

n a spatial setting For example, Using these functions, an inverted index can be created as follows. After reading in a large quantity of web documents, each document must be mapped to a list of $\langle document name, word \rangle$ pairs (where words occur in the documents). 
MapReduce programs typically follow a five-step work flow to solve problems (often graph problems require \emph{multiple rounds}~\cite{ekanayake2010itermapreduce,myung2010sparqlmapreduce}):

\begin{enumerate}
\item Read a large quantity of records
\item Map: extract something of interest from each record, i.e. a list of $\langle k2, v2 \rangle$ pairs. For example $k2$ = document name and $v2$ = a word occurring in the document
\item Shuffle and Sort: gather $v2$ values having the same $k2$ key and send each list to a single reducer 
\item Reduce: aggregate, summarize, filter, or transform all values $v2$ having the same key $k2$.
\end{enumerate}




Two functions, map and reduce, make up the backbone of the programming model in MapReduce:

\begin{tabular}{l l l}
$map$ & $(k1, v1)$ & $\rightarrow list(k2, v2)$\\ 
$reduce$ & (k2, list(v2)) & $\rightarrow list(v2)$ \\
\end{tabular}


While, a developer may use these functions to solve a problem, higher-level languages have been designed that accommodate developers who prefer a different programming style -- including declarative~\cite{thusoo2009hive} and procedural~\cite{olston2008pig,eldawy2014pigeon} languages. Higher-level languages such as HiveQL and Pig are translated into a lower-level program expressed in terms of map and reduce functions.

The primary use case of MapReduce and its derivatives is to process large quantities of semi-structured data, primarily for \emph{data analysis} and for \emph{extract-transform-load} (ETL)~\cite{stonebraker2010friendsorfoes}. In the spatial domain, MapReduce is typically used to perform data analysis~\cite{aji2012largespatial} and to perform . For example, MapReduce can be used to evaluate spatial joins~\cite{zhang2009mapreduce} and to compute geographical map tiles in a scalable manner~\cite{dean2009lessons}.



 (e.g. spatial~{}, graphs and text{}) with several join algorithms implemented and  tasks over large quantities of semi-structured data. The typical problems solved by MapReduce systems involves five step: (1) read a large set of records, (2) \emph{Map}: extract something of interest from each record, (3) Shuffle and Sort (on key)


\chapter{Cloud computing}





\bibliographystyle{plain}
\bibliography{thesis}
                             % Sample .bib file with references that match those in
                             % the 'Specifications Document (V1.5)' as well containing
                             % 'legacy' bibs and bibs with 'alternate codings'.
                             % Gerry Murray - March 2012



Marcos ambitious idea, three parts:
- (I) Introduction: talk about the two chapters that follow. 
- (P1) Pt 1: bulk: declarative cartography, about PRODUCING maps
- (P2) Pt 2: about SERVING digital maps
- Similar structure for both: state of the art, my contributions (advancing the state of the art).  

What-goes-where:
- I <- Case Study, P1 enables P2, Mention where related work is (which parts/chapters)
- P1 <- 
        Survey generalization, 
        CVL1 (closing the gap, part a), 
        CVL2 (closing the gap, part b)
- P2 <- 
        Survey on serving maps, getting to keyed data for maps, all this stuff you can use 
                incl. vectile (step 2),
                caching,
                prediction,
                data partitioning (step 3), 
                replication (step 4), 
                consistency, 
                key-value stores, 
                classic web serving infrastructures,
                papers from seminar
        Gap between state of the art, and the agency
        TileHeat (closing the gap, step 1), deploying a caching infrastructure
        
Publication strategy:

CVL2 (it's a journal thing, not a conference submission):
Approach: "Make CVL more general (star approach), with comparable performance"
- GeoInformatica (extended version of CVL), "completeness/thoroughness": repeat experiments, show new use cases (comp. to CVL), illustrate CVL2 with the new use cases 
- Only conference if "technical fun" or "something new" in compiler, real tech. take-away

\end{document}  

