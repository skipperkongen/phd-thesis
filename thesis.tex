\documentclass[11pt, oneside]{report}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}

% Copied from Marcos Style:
\setlength{\parindent}{0px}
\setlength{\parskip}{1em}

\title{Declarative Multi-Scale Map Visualization and Efficient Serving of Zoomable Geographical Maps}
\author{Pimin Konstantin Kefaloukos}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

\tableofcontents

% STUART: Identifying the problem and objectives
\chapter{Introduction}

%Case Study, P1 enables P2, Mention where related work is (which parts/chapters)

% Emphasise map production and map consumption
\section{Motivation}
% Opportunities for users
\emph{Pannable} and \emph{zoomable} geographical maps are important because they allows users to explore large repositories of information at different focal points and levels of abstraction. Furthermore, \emph{web-based} maps enable online users world-wide to gain spatial insights and make spatial decisions on the go. For example, journalists can enrich online articles with digital maps that yield spatial insights into the complex geographical realities of news stories (e.g. stories about wars, travel or pollution). As another example, tourists can use digital maps and location based services (LBS) to support localized decision making, e.g. finding near-by restaurants, shops or points of interest. Finally, real-estate agents can advertise new deals on geographical maps, which is of great interest to people looking to buy or sell homes in a particular area. % work with the examples

% Challenge 1: too many users
Today, consumers of maps have the opportunity to engage zoomable web maps anytime and anywhere, due in part to advances in mobile technology and a state of near-ubiquitous web access. As a consequence, web-based maps have attracted millions of users who bombard the end-points of spatial data services with a crushing request workload around the clock -- especially during peak hours. TODO: Explain the challenges presented to system engineers further... and give some examples.

% Challenge 2: big data
Everyday, a mass of data is accumulating online, which includes vast amounts of spatial data~\cite{agrawal2012bigdata}. The tendencies of this so-called Big Data are described by the ``three Vs'': \emph{volume}, \emph{variety}, and \emph{velocity}. While the need to visualize much of this data on maps is clear, it is very challenging to do so. Map producers must carefully execute the dual processes of \emph{data abstraction} (e.g. data selection and aggregation)~\cite{haunert2006landcover,schmid2013opensciencemap} and \emph{visual abstraction} (e.g. graphical expression)~\cite{jacques1967semiologie} in order to derive a useful map~\cite{stolte2003multiscale,weibel1999generalising}. These processes are made much harder in the context of Big Data. Furthermore, these tasks generally involve either preprocessing~\cite{sarma2012fusiontables,kefaloukos2014declarative} or indexing~\cite{bereuter2013real,nutanong2012multiresolution} of the data, which also becomes harder for Big Data. TODO: Give some examples

\section{A View of Geospatial Data Systems Today}

%The Shatter and Merge Effect}
% - Shatter: Show that GIS is broken and needs to be shattered (VIZ, OLAP, OLTP)
% - Merge: Show how GIS and cartography has influences other fields:
% - Draw a diagram... that shows how GIS is shattered and visualization is merged

\subsection{The Legacy: Geographical Information Systems}
% The one-size fits all system for spatial data management
% Use cases: analysis, visualization
% This is where we mention: data models, data structures and indexing

% Message: Why we should stop using GIS for everything (reference End of an Era!)
% Who uses GIS: Goverment
% When to use GIS: complex data analysis
% Why GIS is no good: scalable and high-performance online data visualization
% Who should we learn from: Google, Facebook, Amazon (millions of users)
\subsubsection{Data Models}
In geographical information systems, \emph{images} and \emph{objects} are different data models for representing physical or abstract entities on planet Earth and over time. For example, a country is an abstract entity that has an extent on Earth (its borders) and in time (its period of existence), which can be represented either by an image or an object. In the image-based data model time-space is discrete, while in the object-based data model time-space is continuous.

\subsubsection{Data Structures and Indexing}
% linearization, quad-trees, r-trees, 

\subsubsection{Query Models}

\subsection{The Eco-system for Geospatial Data}

% SHATTER
\subsection{Why GIS Does Not Fit All}
% ALTERNATIVE TITLE: Why GIS Needs To Be Shattered
% Use cases: data visualization (DATAVIZ), data creation (OLTP) and data analysis (OLAP)
% - non-experts are increasingly working with data, primarily DATAVIZ and OLTP
% - systems have to handle massive amounts of traffic 24-7-365
%   - need for high performance and high availability
% - massive amonts of web data
% - computers are not getting faster
% neogeography
\subsubsection{The Online Eco-system for Geospatial Data}
% Situation: massive amounts of data, computers not getting faster
% Roles: A pipeline of producers and consumers
% Producers: data creators, visualization designers, application developers
% Consumers: end-users

% Producers: Data Creators and Visualization Designers
% - Who: Public Sector, Private Sector, Citizens (e.g. Crowd-Sourcing)
% - What: Data creation, Visualization Design
% - Why: Governance (Reference Offentlige Data i Spil), Business, Leisure

% Consumers: 24-7, all platforms, a million use-cases

% MERGE
\subsection{Converging Trends in Data Visualization}
% how cartography has influenced data visualization: Information Cartography

\subsection{Requirements for Future Map Systems}
% - Low monetary cost: software and hardware, wages for humans
% - 

\subsection{Research Gaps}
% we need to ENABLE people to: produce data, visualize data
% - Declarative Cartography

% we need to CREATE high-availability and high-performance systems
% - Simple and Effective Data Distribution
% 

% Multi-scale Graph Visualization

\section{Map Services in the Public Sector}
% support for use cases
% high-availability
% high-performance
% consistent and authoritative data
% consistent stale snapshots > in-consistent up-to-date data

\subsection{Case Study: Danish Geodata Agency}

\subsection{Use Cases}

\subsection{Requirements}





% What is the problem... map production and map consumption.

% Views and assumptions
In order to scope our analysis of the complex information pipeline needed to deliver a map, we will need some simplifying \emph{views} and \emph{assumptions}. First, we adopt the view that the mapping pipeline consists of two high-level tasks: \emph{map production} and \emph{map consumption}. Second, we assume that the map production task can be broken down into design substasks  and 

is comprised of two design subtasks: \emph{data abstraction} (e.g. selection and aggregation) and \emph{visual abstraction} (e.g. element styling)~\cite{stolte2003multiscale,jacques1967semiologie}. Third, we assume that map production is  and \emph{preparation} (e.g. preprocessing or indexing). Third, we assume that map consumption occurs in the \emph{client-server model}.

% What are the challenges: caused by high traffic and constant surfacing of new and often big datasets.
XXX challenges of map production are (x), (xx), (xxx) saving manual time, saving computation time, powerful expressibility. YYY challenges of map consumption are (y), (yy), (yyy) high availability and high performance.

% Variables
What are the variables? Relevant metrics should be quantified in service level agreements (SLA) and monitored continuously. For a given fixed SLA, we can mimimize the monetary cost (e.g. salaries, hardware and license expenses, energy bills) of a fixed SLA generally increases with the number of \emph{concurrent users} and data \emph{volume}, \emph{variety}, and \emph{velocity} (``the Three Vs of Big Data'').


% Map production is synonymous with creating good maps, which does not imply consumption of maps. Map consumption is synonymous with users browsing maps on their devices, which does not imply good maps. 
% , with map services running on servers and map clients running on user devices (e.g. mobile devices)

, i.e. , three key challenges face map designers and system engineers: Map designers must (a) address the problem of \emph{data abstraction} (e.g. selection and aggregation) and (b) address the problem of  \emph{visual abstraction} (e.g. choice of graphical styles)~\cite{stolte2003multiscale}. This must be done at the rapid pace that new zoomable maps are needed~\cite{lomet2012warstories}. Third, system engineers must (c) manage \emph{brutal query workloads} (e.g. millions of concurrent users).

\emph{Idea}: motivate preprocessing and vector tiles with data from monitoring on servage that shows that ``dumb'' files have better performance and availability.


\section{Map Design Principles}
In a nutshell, maps must communicate important spatial information in a useful way to their intended audience~\cite{robinson1982early}. While this axiom is certainly true of all maps, it is not concrete enough to effectively guide the map design process. Ideally, it needs to be broken down into a list of requirements that can be checked using quantitative measures. In the following sections, we will review requirements for maps that have been stated.

\subsection{Basic Principles}

\subsubsection{Constant Information Density}
In 1961, the cartographer Friedrich T\"{o}pfer proposed the \emph{Principle of Selection}, which states that the number of objects per display unit should be constant -- regardless of scale~\cite{topfer1966principles}. The soundness of the principle has been verified empirically, e.g. by counting the number of settlements shown per unit display area on different hand-crafted maps of Scotland~\cite{topfer1966principles}. 

In 1998, Woodruff et al. revived the principle for multi-scale database visualizations as the \emph{Principle of Constant Information Density}~\cite{woodruff1998constant}. While these principles differ in name, the core meaning is the same, namely that information density should remain constant over change in scale. A small nuance is in the density metrics that are used, i.e. how information density is quantified.

It is not difficult to understand why the Principle of Constant Information Density must be true. Computer screens and map charts have a fixed area on which to rendering information objects. There is a physical bound on how much information can be represented per unit display area (e.g. a few thousand pixels per $cm^2$ on a computer screen). Also, the human eye cannot see things that are too small or distinguish things that are too close together.

\paragraph{Metrics}

\subsubsection{Distinction}

Implemented as proximity constraint


\subsection{iOS6 Apple Maps Requirements}
Six design objectives have been used to evaluate the iOS6 Apple Maps API~\cite{nutanong2012multiresolution}:
\begin{enumerate}
\item \emph{Minimize overlap}. In later sections show how this requirement is implemented as proximity constraint
\item \emph{Respect relative importance of entries}. Implemented by weighting.
\item \emph{Maximize spatial fullness}. Implemented by SMCP model.
\item \emph{Provide panning/zooming consistency}. Implemented by recursive SQL, ladder approach.
\item \emph{Enable efficient sampling}. Implemented by preprocessing approach.
\item \emph{Support filtering conditions}. Problem here, also exhibited by fusiontables.
\end{enumerate}

\subsection{OpenScienceMaps Requirements}
Check requirementes from OpenScienceMap~\cite{schmid2013opensciencemap}. My notes: Good idea to let rank have impact on proximity constraint: Lower ranked objects require more separation, which implies that conflicts are directed. Question: can the SELECT-PLACES algorithm be implemented in terms of my fundamental constraints and objectives? If not, make sure I can. The places selected by CVL are "too" globally optimal. The important criteria is including locally relevant places, not having a globally optimal result. A five star restaurant should never loose to a four star restaurant!

\subsection{GST Requirements}
Place name implies that we must support star approach as well as ladder approach (se email med Flemming). These will motivate CVL 2.



\subsection{Requirements for iOS 6 Maps}
Apple iPhone 6 maps requirements listed in SELECT-DISTINCT paper, and in duking it out paper. Constant Information density


\section{Overview of State-of-the-Art in Geographical Maps on the Web}
\subsection{Vector Tiles and Client-side Rendering}

\section{Research Gaps}
\subsection{Declarative Design}
\subsection{Workload Prediction}

\section{Contributions of the Dissertation}

\subsection{CVL1}
Brief summary and reference to Section.
\subsection{CVL2}
Brief summary and reference to Section.
\subsection{TileHeat}
Brief summary and reference to Section.


\part{Automatic Multiscale Data Abstraction}

\chapter{Survey of Automatic Multiscale Data Abstraction}

\section{Online processing}

\section{Preprocessing}

\chapter{CVL1}
Closing the gap, part a

\chapter{CVL2} 
Closing the gap, part b

\part{Serving Web Maps}

\chapter{Survey of Map Serving}

\section{Getting to Keyed Data for Maps}

\section{Caching}

\section{Prediction}

\section{Data Partitioning}

\section{Replication}

\section{Consistency}

\section{Key-value Stores}

\section{Classic Web Serving Infrastructures}

\chapter{Case Study: Danish Geodata Agency}

\section{Gap between the State of the Art, and the Agency}

\chapter{TileHeat}
TileHeat (closing the gap, step 1), deploying a caching infrastructure




\chapter{Thesis Topics}

\begin{description}
\item [System Types:] Databases (OLAP, OLTP), ETL, GIS, Data Processing (MapReduce), Caches, Web Servers
\item [Data Models:] Relational, Graph, XML Document, Flat records 
\item [Programming models:] Declarative, Imperiative, Object-Oriented, Functional (MapReduce)
\end{description}

\chapter{Overview of Geographical Maps}

These are the orthogonal dimensions of geographical map design and use.

% Two roles in maps
Two roles in geographical maps: map designer and map user

% Two types of data sources and deployments
Two types of data sources: stored (e.g. database, files) and streaming (e.g. sensors, dynamic databases). Both of these data source types can be local or remote.

% Two types of "liveliness"
Data can be static or dynamic

% Two extreme types of data authoring
Data can be authored automatically (e.g. captured by sensors) or manually (person digitizing a street).

% Two extreme types of map authoring
Maps can be automatically created using algorithms or maps can be manually designed.

% Two types of projections
There are two types of projections: physical (e.g. mountain map) and logical (e.g. subway map).

% Two types of data types
Two types of data types: raster and vector. Two subtypes of raster: orthophoto and grid data. Four subtypes of vector data: points, lines, polygons and polyhedra.

% Two types of end-maps (the stuff that map users look at)
Image-based maps and vector-based maps. Both types can be either single or multi-scale maps.

% Two types of map materializations
Maps can either be materialized off-line or online.

\chapter{Literature Surveys}

Maps There are various paths to create maps.
Follow a (1) design map; (2) create map ; (3) serve map approach, especially with regard to survey topics covered. Each technology will be ranked in these three categories using a set of metrics: (a) monetary cost; (b) scalability; (c) availability; (4) ease of use.


\section{MapReduce for Maps}
% In this section I'll survey how Map Reduce is used for spatial data.


% Typical problem solved by MapReduce
% - Read a lot of data
% - Map: extract something you care about from each record
% - Shuffle and Sort
% - Reduce: aggregate, summarize, filter, or transform
% - Write the results
% Outline stays the same, map and reduce change to fit the problem
% \cite{dean2009lessons}
MapReduce is a programming model and parallel data processing system implemented by Google and later described in a highly-cited white paper~\cite{dean2008mapreduce}. While MapReduce was originally designed by Google to run an its shared-nothing cluster of thousands of commodity servers, it is now deployed on both shared-memory multicores~\cite{ranger2007multicore} and graphics processors (GPUs)~\cite{he2008mapreducegpu} and used by some of the worlds largest organizations. The wide-spread adoption of the MapReduce programming model is largely due to the open source Apache Hadoop implementation~\cite{apachehadoop}, which together with the simplicity of the programming mode has made it a popular choice for scalable data processing. 

The backbone of a MapReduce program is made up of two functions, map and reduce:

\begin{tabular}{l l l}
$map$ & $(k1, v1)$ & $\rightarrow list(k2, v2)$\\ 
$reduce$ & (k2, list(v2)) & $\rightarrow list(v2)$ \\
\end{tabular}

As a natural consequence of its popularity, MapReduce has been used for spatial use cases, e.g. for spatial data analytics and spatial Extract-Transform-Load (ETL). For example, MapReduce can be used as an ETL tool to compute a set of geographical map tiles from a spatial dataset (points, lines, polygons) before loading the tiles into a scalable key-value store~\cite{dean2009lessons}. This follows a five-step work flow typical of MapReduce programs (we will assume $n$ mappers and $m$ reducers):

\begin{enumerate}
\item The set of spatial features is randomly partitioned $n$ ways and forwarded to the $n$ mappers

\item For each feature, the map function computes the set of intersected map tiles (rectangular coordinate region) and emits a list of corresponding $\langle TileID, Feature \rangle$ pairs

\item In a shuffle and sort step, the $\langle TileID, Feature \rangle$ pairs are rearranged into lists such that contain all $Feature$ values that were grouped with a given $TileID$ key, and these lists are distributed evenly among the $m$ reducers

\item Each reducer receives a set of $\langle TileID, List(Feature) \rangle$ pairs and aggregates each $Feature$ list into a finished map tile for $TileID$

\item The set of map tiles is stored on a high-performance key-value store, ready for client map requests. 
\end{enumerate}

\subsection{MapReduce Languages}

While developers can directly write basic MapReduce code to solve a large class of spatial analytics problems (CITE SPATIAL JOIN IN MAP REDUCE), languages have been designed that accommodate developers who prefer a different programming style -- including higher-level declarative~\cite{thusoo2009hive} and procedural~\cite{olston2008pig,eldawy2014pigeon} styles. Higher-level languages, like HiveQL and Pig, are translated into lower-level programs expressed in terms of the map and reduce functions. These languages are part of database systems that are built on top of MapReduce, such as HadoopDB~\cite{abouzeid2009hadoopdb}, HadoopGIS~\cite{aji2013hadoopgis}, Pig~\cite{eldawy2014pigeon, olston2008pig} and Hive~\cite{thusoo2009hive}.

\subsection{MapReduce for Materializing Maps}

A parallel RDBMS (e.g. an in-memory column-store) is generally the most efficient way to analyze large datasets~\cite{pavlo2009comparison}. For this reason several large organizations including Google are now switching back to an RDBMS approach for their analytics (away from MapReduce)~\cite{melnik2011dremel}. This switch will probably remain sensible from a pure performance point of view, in spite of recent research effort into automatically optimizing MapReduce ~\cite{jahani2011mapreduceoptimization,floratou2011columnmapreduce}. Instead of competing with a parallel RDBMS for the spot as analytics engine, MapReduce will most likely play a role together with a parallel RDMBS in an composite analytics architecture, i.e. as an \emph{extract-transform-load} (ETL) tool in front of a parallel RDBMS~\cite{stonebraker2010friendsorfoes}. The previously mentioned case of generating map tiles using MapReduce is an example of using MapReduce as an ETL tool in-front of a key-value store.


While MapReduce frameworks can be used to implement the guts of a database, an recent trend is to use MapReduce as an  Several large organizations including Google have taking this route lately, and in a sense 


In a spatial setting, geographical map tiles can be created using MapReduce by following a five-step work flow that is typical of such MapReduce programs. 

n a spatial setting For example, Using these functions, an inverted index can be created as follows. After reading in a large quantity of web documents, each document must be mapped to a list of $\langle document name, word \rangle$ pairs (where words occur in the documents). 
MapReduce programs typically follow a five-step work flow to solve problems (often graph problems require \emph{multiple rounds}~\cite{ekanayake2010itermapreduce,myung2010sparqlmapreduce}):

\begin{enumerate}
\item Read a large quantity of records
\item Map: extract something of interest from each record, i.e. a list of $\langle k2, v2 \rangle$ pairs. For example $k2$ = document name and $v2$ = a word occurring in the document
\item Shuffle and Sort: gather $v2$ values having the same $k2$ key and send each list to a single reducer 
\item Reduce: aggregate, summarize, filter, or transform all values $v2$ having the same key $k2$.
\end{enumerate}




Two functions, map and reduce, make up the backbone of the programming model in MapReduce:

\begin{tabular}{l l l}
$map$ & $(k1, v1)$ & $\rightarrow list(k2, v2)$\\ 
$reduce$ & (k2, list(v2)) & $\rightarrow list(v2)$ \\
\end{tabular}


While, a developer may use these functions to solve a problem, higher-level languages have been designed that accommodate developers who prefer a different programming style -- including declarative~\cite{thusoo2009hive} and procedural~\cite{olston2008pig,eldawy2014pigeon} languages. Higher-level languages such as HiveQL and Pig are translated into a lower-level program expressed in terms of map and reduce functions.

The primary use case of MapReduce and its derivatives is to process large quantities of semi-structured data, primarily for \emph{data analysis} and for \emph{extract-transform-load} (ETL)~\cite{stonebraker2010friendsorfoes}. In the spatial domain, MapReduce is typically used to perform data analysis~\cite{aji2012largespatial} and to perform . For example, MapReduce can be used to evaluate spatial joins~\cite{zhang2009mapreduce} and to compute geographical map tiles in a scalable manner~\cite{dean2009lessons}.



 (e.g. spatial~{}, graphs and text{}) with several join algorithms implemented and  tasks over large quantities of semi-structured data. The typical problems solved by MapReduce systems involves five step: (1) read a large set of records, (2) \emph{Map}: extract something of interest from each record, (3) Shuffle and Sort (on key)


\chapter{Cloud computing}





\bibliographystyle{plain}
\bibliography{thesis}
                             % Sample .bib file with references that match those in
                             % the 'Specifications Document (V1.5)' as well containing
                             % 'legacy' bibs and bibs with 'alternate codings'.
                             % Gerry Murray - March 2012



Marcos ambitious idea, three parts:
- (I) Introduction: talk about the two chapters that follow. 
- (P1) Pt 1: bulk: declarative cartography, about PRODUCING maps
- (P2) Pt 2: about SERVING digital maps
- Similar structure for both: state of the art, my contributions (advancing the state of the art).  

What-goes-where:
- I <- Case Study, P1 enables P2, Mention where related work is (which parts/chapters)
- P1 <- 
        Survey generalization, 
        CVL1 (closing the gap, part a), 
        CVL2 (closing the gap, part b)
- P2 <- 
        Survey on serving maps, getting to keyed data for maps, all this stuff you can use 
                incl. vectile (step 2),
                caching,
                prediction,
                data partitioning (step 3), 
                replication (step 4), 
                consistency, 
                key-value stores, 
                classic web serving infrastructures,
                papers from seminar
        Gap between state of the art, and the agency
        TileHeat (closing the gap, step 1), deploying a caching infrastructure
        
Publication strategy:

CVL2 (it's a journal thing, not a conference submission):
Approach: "Make CVL more general (star approach), with comparable performance"
- GeoInformatica (extended version of CVL), "completeness/thoroughness": repeat experiments, show new use cases (comp. to CVL), illustrate CVL2 with the new use cases 
- Only conference if "technical fun" or "something new" in compiler, real tech. take-away

\end{document}  

